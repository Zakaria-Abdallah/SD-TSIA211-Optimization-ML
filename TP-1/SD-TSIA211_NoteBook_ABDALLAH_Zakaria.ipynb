{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SD-TSIA211:TP1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $Ax = b$, we aim to demonstrate that $y(t) = \\frac{1}{2} \\tilde{x}(t)^T w_2 + \\tilde{x}(t)^T w_1 + w_0$.\n",
    "\n",
    "Given the definition of $A$ and $w$, and the fact that $Ax = b$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{x}(t)^T w_1 + w_0 &= y(t) - y(t) \\times \\tilde{x}(t)^T w_2 \\\\\n",
    "&= y(t) - \\tilde{x}(t)^T (Aw)_2 \\\\\n",
    "&= y(t) \\text{ for each } t.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Rearranging this equation:\n",
    "\n",
    "$$\n",
    "\\tilde{x}(t)^T w_1 + w_0 = y(t) + y(t) \\times \\tilde{x}(t)^T w_2\n",
    "$$\n",
    "\n",
    "Bringing $y(t)$ terms together:\n",
    "\n",
    "$$\n",
    "\\tilde{x}(t)^T w_1 + w_0 = y(t) (1 + \\tilde{x}(t)^T w_2)\n",
    "$$\n",
    "\n",
    "And isolating $y(t)$:\n",
    "\n",
    "$$\n",
    "y(t) = \\frac{\\tilde{x}(t)^T w_1 + w_0}{1 + \\tilde{x}(t)^T w_2}\n",
    "$$\n",
    "\n",
    "This demonstrates that if $Ax = b$, then the relationship $y(t) = \\frac{1}{2} \\tilde{x}(t)^T w_2 + \\tilde{x}(t)^T w_1 + w_0$ is indeed valid.\n",
    "\n",
    "\n",
    "The dot product $ \\mathbf{w}_1^T \\tilde{\\mathbf{x}}(t) $\n",
    "\n",
    "- $ \\mathbf{w}_1^T $ is the transpose of the weight vector $ \\mathbf{w}_1 $, making it a row vector.\n",
    "- $ \\tilde{\\mathbf{x}}(t) $ is a column vector of standardized measurements at time t\n",
    "- The dot product $ \\mathbf{w}_1^T \\tilde{\\mathbf{x}}(t) $ is a scalar, representing a weighted sum of the measurements.\n",
    "\n",
    "The dot product $ \\tilde{\\mathbf{x}}(t)^T \\mathbf{w}_1 $\n",
    "\n",
    "- $ \\tilde{\\mathbf{x}}(t)^T $ is the transpose of $ \\tilde{\\mathbf{x}}(t) $ making it a row vector.\n",
    "- $ \\mathbf{w}_1 $ is a column vector.\n",
    "- The dot product $ \\tilde{\\mathbf{x}}(t)^T \\mathbf{w}_1 $ is also a scalar, and due to the properties of transpose and dot product, it's equal to $ \\mathbf{w}_1^T \\tilde{\\mathbf{x}}(t) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)\n",
    "\n",
    "# Constructing matrices for min_w ||A w - b||_2**2\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T])\n",
    "b = COP_train[:,3]\n",
    "\n",
    "\n",
    "\n",
    "# Constructing matrices for the test set\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test = np.hstack([M_test, np.ones((M_test.shape[0],1)), -(M_test.T * COP_test[:,3]).T])\n",
    "b_test = COP_test[:,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [-0.00927821  0.08309371 -0.03672704 ...  0.01980595 -0.03057174\n",
      " -0.01188614]\n"
     ]
    }
   ],
   "source": [
    "w, residuals, _, _ = np.linalg.lstsq(A, b, rcond=None)\n",
    "print(\"Weights:\", w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on Test Set: -40.87585582906962\n",
      "Mean Squared Error on Test Set: 390.44923967618354\n"
     ]
    }
   ],
   "source": [
    "predictions = np.dot(A_test, w)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "mse = 0.5*mean_squared_error(b_test, predictions)\n",
    "print(f\"R-squared on Test Set: {r2_score(b_test, predictions)}\")\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{x} \\frac{1}{2} \\|Ax - b\\|^2_2 + \\frac{\\lambda}{2} \\|x\\|^2_2\n",
    "$$\n",
    "\n",
    "Expands to:\n",
    "\n",
    "$$\n",
    "\\min_{x} \\frac{1}{2} (Ax - b)^T(Ax - b) + \\frac{\\lambda}{2} x^T x\n",
    "$$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$$\n",
    "\\min_{x} \\frac{1}{2} (x^T A^T Ax - x^T A^T b - b^T Ax + b^T b) + \\frac{\\lambda}{2} x^T x\n",
    "$$\n",
    "\n",
    "Modified Form:\n",
    "\n",
    "$$\n",
    "\\min_{x} \\left\\| \\begin{pmatrix} A \\\\ \\sqrt{\\lambda}I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\|_2^2\n",
    "$$\n",
    "\n",
    "Expands to:\n",
    "\n",
    "$$\n",
    "\\min_{x} \\left( \\begin{pmatrix} A \\\\ \\sqrt{\\lambda}I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right)^T \\left( \\begin{pmatrix} A \\\\ \\sqrt{\\lambda}I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right)\n",
    "$$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$$\n",
    "\\min_{x} \\left( x^T \\begin{pmatrix} A^T & \\sqrt{\\lambda}I \\end{pmatrix} \\begin{pmatrix} A \\\\ \\sqrt{\\lambda}I \\end{pmatrix} x - 2x^T \\begin{pmatrix} A^T & \\sqrt{\\lambda}I \\end{pmatrix} \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} b \\\\ 0 \\end{pmatrix}^T \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\min_{x} \\left( x^T (A^T A + \\lambda I) x - 2x^T A^T b + b^T b \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on Test Set: -15.144132574526477\n",
      "Regularized Mean Squared Error on Test Set: 150.5274140470126\n"
     ]
    }
   ],
   "source": [
    "lambda_value = 100.0\n",
    "\n",
    "A_regularized = np.vstack([A, np.sqrt(lambda_value) * np.eye(A.shape[1])])\n",
    "b_regularized = np.concatenate([b, np.zeros(A.shape[1])])\n",
    "\n",
    "\n",
    "w_regularized, _, _, _ = np.linalg.lstsq(A_regularized, b_regularized, rcond=None)\n",
    "\n",
    "\n",
    "predictions_regularized = np.dot(A_test, w_regularized)\n",
    "mse_regularized = 0.5*mean_squared_error(b_test, predictions_regularized)\n",
    "\n",
    "print(f\"R-squared on Test Set: {r2_score(b_test, predictions_regularized)}\")\n",
    "print(f\"Regularized Mean Squared Error on Test Set: {mse_regularized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noticed that the test mean square error with the unregularized one is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function $f_1(w) = \\frac{1}{2} \\| Aw - b \\|^2_2 + \\frac{\\lambda}{2} \\| w \\|^2_2$, we calculate its gradient as follows:\n",
    "\n",
    "1. **Gradient of the First Term:** The first term is a least squares term. The gradient of $\\frac{1}{2} \\| Aw - b \\|^2_2$ with respect to $w$ is:\n",
    "\n",
    "   $$\n",
    "   \\nabla_w \\left( \\frac{1}{2} \\| Aw - b \\|^2_2 \\right) = A^T (Aw - b)\n",
    "   $$\n",
    "\n",
    "2. **Gradient of the Second Term:** The second term is the L2 regularization term. The gradient of $\\frac{\\lambda}{2} \\| w \\|^2_2$ with respect to $w$ is:\n",
    "\n",
    "   $$\n",
    "   \\nabla_w \\left( \\frac{\\lambda}{2} \\| w \\|^2_2 \\right) = \\lambda w\n",
    "   $$\n",
    "\n",
    "Therefore, the total gradient of $f_1(w)$ is the sum of these two gradients:\n",
    "\n",
    "$$\n",
    "\\nabla_w f_1(w) = A^T (Aw - b) + \\lambda w\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is convex, sum of two convex functions is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use constant step size, the step size if the inverse of Lipschitz constant:\n",
    "\n",
    "Given the function $$ f(w) = \\frac{1}{2} \\| Aw - b \\|^2_2 + \\frac{\\lambda}{2} \\| w \\|^2_2 $$, we want to find the Lipschitz constant $$ L $$ for the gradient of this function. The gradient is given by:\n",
    "\n",
    "$$\n",
    "\\nabla f(w) = A^T (Aw - b) + \\lambda w.\n",
    "$$\n",
    "\n",
    "To find the Lipschitz constant \\( L \\) for this gradient, we need a constant such that for all \\( w_1, w_2 \\),\n",
    "\n",
    "$$\n",
    "\\| \\nabla f(w_1) - \\nabla f(w_2) \\| \\leq L \\| w_1 - w_2 \\|.\n",
    "$$\n",
    "\n",
    "Let's calculate \\( \\nabla f(w_1) - \\nabla f(w_2) \\):\n",
    "\n",
    "$$\n",
    "\\nabla f(w_1) - \\nabla f(w_2) = (A^T A w_1 + \\lambda w_1) - (A^T A w_2 + \\lambda w_2) = A^T A (w_1 - w_2) + \\lambda (w_1 - w_2).\n",
    "$$\n",
    "\n",
    "Now, using the property of matrix norms:\n",
    "\n",
    "$$\n",
    "\\| A^T A (w_1 - w_2) + \\lambda (w_1 - w_2) \\| \\leq \\| A^T A (w_1 - w_2) \\| + \\| \\lambda (w_1 - w_2) \\|.\n",
    "$$\n",
    "\n",
    "Using the sub-multiplicative property of norms:\n",
    "\n",
    "$$\n",
    "\\| A^T A (w_1 - w_2) \\| \\leq \\| A^T A \\| \\| w_1 - w_2 \\|, \\quad \\| \\lambda (w_1 - w_2) \\| = \\lambda \\| w_1 - w_2 \\|.\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\| A^T A (w_1 - w_2) + \\lambda (w_1 - w_2) \\| \\leq (\\| A^T A \\| + \\lambda) \\| w_1 - w_2 \\|.\n",
    "$$\n",
    "\n",
    "Therefore, the Lipschitz constant \\( L \\) is the largest singular value of $A^T A$ (which is the square of the largest singular value of A, or the spectral norm of $ A^T A $ plus $ \\lambda $:\n",
    "\n",
    "$$\n",
    "L = \\| A^T A \\| + \\lambda.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.33454875565108\n",
      "20941\n",
      "[-0.01425106  0.05605583  0.00349638 ...  0.01356557 -0.04187102\n",
      "  0.02009826]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "product_matrix = np.dot(np.transpose(A), A)\n",
    "eigenvalues, _ = np.linalg.eig(product_matrix)\n",
    "max_eigenvalue = np.max(np.real(eigenvalues))\n",
    "\n",
    "\n",
    "Lipschitz_constatnt = max_eigenvalue + lambda_value\n",
    "step_size = 1 / Lipschitz_constatnt\n",
    " \n",
    "def gradient_f1(A, b, w, lambda_value):\n",
    "    return np.dot(A.T, np.dot(A, w) - b) + lambda_value * w\n",
    "\n",
    "w_k = np.zeros(A.shape[1])\n",
    "start_time = time.time()\n",
    "iteration = 0\n",
    "while time.time() - start_time < 30:\n",
    "    gradient = gradient_f1(A, b, w_k, lambda_value)\n",
    "    if np.linalg.norm(gradient) <= 1:\n",
    "        print(f\"Converged after {iteration + 1} iterations.\")\n",
    "        iteration += 1\n",
    "        break\n",
    "    w_k = w_k - step_size * gradient\n",
    "    iteration += 1\n",
    "print(np.linalg.norm(gradient))\n",
    "print(iteration)\n",
    "print(w_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use exact line search to calculate the step size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20836\n",
      "[-0.01376004  0.05643976  0.00263156 ...  0.01405717 -0.04075652\n",
      "  0.01924853]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def ridge_regression_loss(x, A, b, lambda_reg):\n",
    "    least_squares_loss = 0.5 * np.sum((A @ x - b) ** 2)\n",
    "    regularization_term = lambda_reg * np.sum(x ** 2)\n",
    "    return least_squares_loss + regularization_term\n",
    "\n",
    "def ridge_regression_gradient(x, A, b, lambda_reg):\n",
    "    gradient = A.T @ (A @ x - b) + 2 * lambda_reg * x\n",
    "    return gradient\n",
    "\n",
    "def function_to_minimize(t, x, A, b, lambda_reg):\n",
    "    grad = ridge_regression_gradient(x, A, b, lambda_reg)\n",
    "    return ridge_regression_loss(x - t * grad, A, b, lambda_reg)\n",
    "\n",
    "w_k = np.zeros(A.shape[1])\n",
    "start_time = time.time()\n",
    "\n",
    "res = minimize_scalar(function_to_minimize, args=(w_k, A, b, lambda_value))\n",
    "step_size_ = res.x\n",
    "iteration = 0\n",
    "while time.time() - start_time < 30:\n",
    "    gradient = gradient_f1(A, b, w_k, lambda_value)\n",
    "    if np.linalg.norm(gradient) <= 1:\n",
    "        print(f\"Converged after {iteration + 1} iterations.\")\n",
    "        iteration += 1\n",
    "        break\n",
    "    w_k = w_k - step_size_ * gradient\n",
    "    iteration += 1\n",
    "print(iteration)\n",
    "print(w_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function $$ F_2(w) = \\frac{1}{2} \\| Aw - b \\|^2_2 + \\lambda \\| w \\|_1 $$, we split it into  $f_2 $ and $ g_2 $:\n",
    "\n",
    "1. **Differentiable Part ($ f_2 $)**: This is the least squares term, which is smooth and differentiable.\n",
    "   $$\n",
    "   f_2(w) = \\frac{1}{2} \\| Aw - b \\|^2_2\n",
    "   $$\n",
    "\n",
    "2. **Non-Differentiable Part ($ g_2 $)**: This is the L1 regularization term, which is not differentiable at 0 but has a simple proximal operator.\n",
    "   $$\n",
    "   g_2(w) = \\lambda \\| w \\|_1\n",
    "   $$\n",
    "\n",
    "**Calculating the Gradient of $ f_2 $**\n",
    "\n",
    "The gradient of $ f_2(w) $ with respect to $ w $ is:\n",
    "   $$\n",
    "   \\nabla f_2(w) = A^T (Aw - b)\n",
    "   $$\n",
    "\n",
    "**Proximal Operator of $ g_2 $**\n",
    "\n",
    "The proximal operator for the L1 norm (which corresponds to $ g_2(w) = \\lambda \\| w \\|_1 $) is the soft thresholding operator. For a given vector $w $ and a scalar $ t $, the proximal operator $ \\text{prox}_{t g_2} $ is defined as:\n",
    "   $$\n",
    "   \\text{prox}_{t g_2}(w)_i = \\text{sign}(w_i) \\max(|w_i| - \\lambda t, 0)\n",
    "   $$\n",
    "for each component $ w_i $ of the vector $ w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4092.2498839193945\n",
      "[-0.          0.00370377  0.00132033 ... -0.          0.00097475\n",
      " -0.0008736 ]\n"
     ]
    }
   ],
   "source": [
    "def objective_function(x):\n",
    "    return 0.5 * np.linalg.norm(b - A @ x) ** 2 +  lambda_reg * np.linalg.norm(x, 1)\n",
    "def proxi(w, lambda_t):\n",
    "    return np.sign(w) * np.maximum(np.abs(w) - lambda_t, 0)\n",
    "\n",
    "def proximal_gradient_descent(A, b, lambda_reg, step_size_lasso, max_iter=100, tol=1e-6):\n",
    "    w_k_lasso= np.zeros(A.shape[1])\n",
    "    for i in range(max_iter):\n",
    "        grad = A.T @ (A @ w_k_lasso - b)\n",
    "        w_new = proxi(w_k_lasso - step_size_lasso * grad, step_size_lasso * lambda_reg)  \n",
    "        if np.linalg.norm(w_new - w_k_lasso) < tol:\n",
    "            break\n",
    "        w_k_lasso = w_new\n",
    "    return w_k_lasso\n",
    "\n",
    "lambda_reg = 200\n",
    "step_size_lasso = 1 / max_eigenvalue\n",
    "\n",
    "w_lasso = proximal_gradient_descent(A, b, lambda_reg, step_size_lasso)\n",
    "min_value1 = objective_function(w_lasso)\n",
    "print(min_value1)\n",
    "print(w_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7463.867958769965\n",
      "[-0.00359126  0.01788695  0.00991634 ...  0.00162506 -0.00429348\n",
      " -0.00084256]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 0.5 * np.linalg.norm(b - A @ x) ** 2\n",
    "def line_search(x, _grad, _a=1/2, _b=1, _beta = 1/2):\n",
    "    l = 0\n",
    "    _gamma = _b*_a**l\n",
    "    x_plus = x - _gamma*_grad\n",
    "    while f(x_plus) > f(x) + _beta* (_grad @ (x_plus-x)) :\n",
    "        l += 1\n",
    "        _gamma = _b*_a**l\n",
    "        x_plus = x - _gamma*_grad\n",
    "    return l\n",
    "\n",
    "def proximal_gradient_descent(x0, eps = 1e-5, max_iter = 100):\n",
    "    x = x0\n",
    "    arr_zeros = np.zeros(x.shape)\n",
    "    _iter = 0\n",
    "    rho = 0.02\n",
    "    norm_arr = []\n",
    "    a, b , beta = 1/2, 1, 1/2\n",
    "    grad = grad_f(x)\n",
    "    norm = np.linalg.norm(grad,2)\n",
    "    norm_arr.append(norm)\n",
    "    \n",
    "    while (norm > eps) & (_iter <= max_iter):\n",
    "        _iter += 1\n",
    "        gamma = b * a**line_search(x, grad)\n",
    "        xi = x - gamma*grad\n",
    "        x = np.sign(xi)*np.maximum(np.abs(xi)-rho*gamma,arr_zeros)\n",
    "        grad = grad_f(x)\n",
    "        norm = np.linalg.norm(grad,2)\n",
    "        norm_arr.append(norm)\n",
    "\n",
    "    \n",
    "    return x\n",
    "\n",
    "def grad_f(w_k_lasso):\n",
    "    return A.T @ (A @ w_k_lasso - b)\n",
    "    \n",
    "\n",
    "lambda_reg = 200\n",
    "learning_rate =  1 / max_eigenvalue\n",
    "w_k_lasso_line = np.zeros(A.shape[1])\n",
    "w_line_search = proximal_gradient_descent(w_k_lasso_line, 1e-5, 100)\n",
    "min_value2 = objective_function(w_k_lasso_line)\n",
    "print(min_value2)\n",
    "print(w_line_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 5: \n",
    "\n",
    "we do not implement the method but we give some important information that express our understanding here we go :\n",
    "\n",
    "In our machine learning problem, the primary goal is to find the optimal regularization parameter $\\rho$ that achieves a balance between reducing model complexity and ensuring good generalization on new data. This is approached through a bilevel optimization framework:\n",
    "\n",
    "Inner Level: Minimize a regularized loss function on the training dataset $(X_{\\text{train}}, y_{\\text{train}})$. The loss function combines a predictive error term (like mean square error) and a regularization term. Two types of regularization are considered: L2 (Ridge, $\\frac{1}{2} \\|w\\|_2^2$) and L1 (Lasso, $\\|w\\|_1$), where $w$ represents model parameters.\n",
    "\n",
    "Outer Level: Maximize the model's performance on a separate validation set $(X_{\\text{valid}}, y_{\\text{valid}})$ by selecting the best $\\rho$. The performance is measured using a loss function, typically the mean square error.\n",
    "\n",
    "Grid Search Method: Due to the complexity of the optimization problem, you employ a grid search to find the best $\\rho$. You systematically evaluate a range of $\\rho$ values generated by $\\{\\rho_0 \\cdot a^k : k \\in \\{0,1,\\ldots,K\\}\\}$, where $\\rho_0$ is a positive base value, $a$ is a factor less than 1, and $K$ is an integer defining the grid's extent.\n",
    "\n",
    "Evaluation and Selection: For each $\\rho$ in the grid, the model is trained (inner optimization), and its performance is assessed on the validation set. The $\\rho$ value (and corresponding model parameters $w$) that leads to the lowest validation loss is selected as the optimal choice.\n",
    "\n",
    "This approach effectively balances the trade-offs inherent in regularized regression problems, optimizing model complexity for better generalization while also managing the computational demands of the optimization process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 6:\n",
    "\n",
    "based on our run that we do and by examining the result using different method we can make a comparison as follow:\n",
    "\n",
    "**Optimal Weights:**\n",
    "\n",
    "L1 Regularization tends to produce sparse solutions with some weights exactly zero, enhancing model interpretability through feature selection.\n",
    "\n",
    "L2 Regularization typically results in non-sparse solutions where weights are reduced but not zeroed, retaining all features but with less interpretability.\n",
    "\n",
    "**Convergence Speed:**\n",
    "\n",
    "L1 Regularization generally converges faster, especially with streamlined optimization techniques.\n",
    "\n",
    "L2 Regularization often requires more iterations to converge, reflecting its continuous, gradual weight reduction.\n",
    "\n",
    "**Interpretability and Overfitting:**\n",
    "\n",
    "L1 is preferable for simpler, more interpretable models, useful in scenarios where understanding feature influence is key.\n",
    "\n",
    "L2, by retaining all features with smaller weights, reduces overfitting risk but at the cost of lower interpretability.\n",
    "\n",
    "**Algorithm Efficiency:**\n",
    "\n",
    "The efficiency of L1 can be influenced by the choice of optimization techniques, with some methods accelerating convergence.\n",
    "\n",
    "L2's slower convergence might impact computational efficiency, especially in larger datasets or complex models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
